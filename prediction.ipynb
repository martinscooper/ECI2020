{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from category_encoders import TargetEncoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "sns.set()\n",
    "pd.set_option('display.max_rows', 90)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discard_not_closed(df):\n",
    "    return df.loc[df['Stage'].apply(lambda x: x in ['Closed Won', 'Closed Lost'])]\n",
    "\n",
    "def process_data(df, with_leackage=False):\n",
    "    \n",
    "    binary_cols = ['Pricing, Delivery_Terms_Quote_Appr', 'Pricing, Delivery_Terms_Approved', \n",
    "               'Bureaucratic_Code_0_Approval','Bureaucratic_Code_0_Approved']\n",
    "\n",
    "    categorical_cols = ['Region','Territory','Bureaucratic_Code', \n",
    "                    'Source ','Billing_Country','Account_Name',\n",
    "                    'Opportunity_Name', 'Account_Owner',\n",
    "                   'Opportunity_Owner','Account_Type', 'Opportunity_Type', \n",
    "                    'Quote_Type', 'Delivery_Terms', 'Brand', 'Product_Type', \n",
    "                    'Size','Product_Category_B', 'Currency', 'Last_Modified_By',\n",
    "                    'Product_Family', 'Product_Name', 'ASP_Currency',\n",
    "                    'ASP_(converted)_Currency','Delivery_Quarter',\n",
    "                    'Total_Taxable_Amount_Currency',\n",
    "                    'Prod_Category_A', 'Total_Amount_Currency'\n",
    "                   ]\n",
    "    float_cols = ['ASP', 'ASP_(converted)', \n",
    "                  'TRF', 'Total_Amount', \n",
    "                  'Total_Taxable_Amount', 'ID']\n",
    "\n",
    "\n",
    "    datetime_cols = ['Account_Created_Date', 'Opportunity_Created_Date',\n",
    "                 'Last_Activity','Quote_Expiry_Date', 'Last_Modified_Date',\n",
    "                 'Planned_Delivery_Start_Date', 'Planned_Delivery_End_Date',\n",
    "                 'Actual_Delivery_Date', \n",
    "                ]\n",
    "    other_cols = ['Month', 'Delivery_Year', 'Price']\n",
    "\n",
    "    target_col = ['Stage']\n",
    "\n",
    "    ######### debe ser eliminada ya que aumentara el desempeÃ±o del predictor ahora pero en\n",
    "    ######### un caso real nunca se tendra el valor de esta variable ya que parece ser seteado\n",
    "    ######### una vez que la oportunidad resulta en exito.\n",
    "    target_leakage = ['Sales_Contract_No']\n",
    "    # Tipos de variables segun la decr del dataset\n",
    "    \n",
    "    # Saco algunas columnas\n",
    "    # demasiados categorias\n",
    "    many_cats = ['Opportunity_Name']\n",
    "    # por gran mayoria de nones:\n",
    "    nones_may = ['Product_Type', 'Product_Category_B','Currency', 'Size', 'Brand']\n",
    "    # por similitud a ASP_Currency\n",
    "    sim_vals = ['Total_Taxable_Amount_Currency'] \n",
    "    # posee un solo valor o casi\n",
    "    one_val = ['Prod_Category_A', 'ASP_(converted)_Currency','Quote_Type', 'Submitted_for_Approval']\n",
    "    # Id columns\n",
    "    id_cols = ['ID']\n",
    "    #datetime nones\n",
    "    none_date = ['Last_Activity','Actual_Delivery_Date']\n",
    "\n",
    "    \n",
    "    def difference_list(l1,l2):\n",
    "        return [x for x in l1 if x not in l2]\n",
    "\n",
    "    categorical_cols = difference_list(categorical_cols, nones_may + sim_vals + one_val + many_cats)\n",
    "    float_cols = difference_list(float_cols, id_cols + target_leakage)\n",
    "    datetime_cols = difference_list(datetime_cols, none_date)\n",
    "    \n",
    "    if with_leackage:\n",
    "        categorical_cols += target_leakage\n",
    "    \n",
    "    df = df.loc[:, categorical_cols + binary_cols + float_cols + datetime_cols]\n",
    "    # Completo Nones\n",
    "    df.loc[df['Territory']=='None', 'Territory'] = df.loc[df['Territory']=='None', 'Region'] \n",
    "\n",
    "    # Completo NaN\n",
    "    df['ASP_(converted)'].fillna(0, inplace=True)\n",
    "    df['ASP'].fillna(0, inplace=True)\n",
    "    df['Total_Amount'].fillna(0, inplace=True)\n",
    "\n",
    "    # Convierto los tipos de variables segun la descr. del df\n",
    "    df.loc[:,binary_cols] = df.loc[:,binary_cols].astype('bool')\n",
    "    df.loc[:,categorical_cols] = df.loc[:,categorical_cols].astype('object')\n",
    "    df.loc[:,float_cols] = df.loc[:,float_cols].astype('float64')\n",
    "    #df[target_leakage] = df[target_leakage].astype('int')\n",
    "    for col in datetime_cols:\n",
    "        df.loc[:,col] = pd.to_datetime(df.loc[:,col])\n",
    "    \n",
    "    #completo end delivery date con la suma entre Planned_Delivery_End_Date y el promedio de su diferencia con Planned_Delivery_End_Date\n",
    "    diff_delivery = df['Planned_Delivery_End_Date'] - df['Planned_Delivery_Start_Date']\n",
    "    start_plus_mean = df.loc[df['Planned_Delivery_End_Date'].isnull(),'Planned_Delivery_Start_Date'] + diff_delivery.mean()\n",
    "    df.loc[df['Planned_Delivery_End_Date'].isnull(), 'Planned_Delivery_End_Date'] = start_plus_mean\n",
    "    \n",
    "    \n",
    "    #preparo los grupos segun la oportunidad y\n",
    "    # un grupo con la primer columna de cada grupo\n",
    "    groups = df.groupby(df.index)\n",
    "    df_first = df.reset_index().drop_duplicates(subset='Opportunity_ID', keep='first').set_index('Opportunity_ID')\n",
    "    \n",
    "    df_count = groups.size().to_frame('nb_products')\n",
    "    df_count.loc[:, 'only_one_product'] = df_count['nb_products'] == 1 \n",
    "    \n",
    "    \n",
    "    groups_float = groups[df.select_dtypes(include=['float','int']).columns]\n",
    "    df_float = groups_float.agg(['mean','std','min','max','sum']).fillna(0)\n",
    "    \n",
    "    def sin_cos_date(df, col, total):\n",
    "        df[col + '_sin'] = np.sin(2 * np.pi * df[col] / total)\n",
    "        df[col + '_cos'] = np.cos(2 * np.pi * df[col] / total)\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    datetime_cols = df.select_dtypes(include='datetime').columns\n",
    "    df_datetime = df_first.loc[:, datetime_cols]\n",
    "    \n",
    "    #datetime vars engineer\n",
    "    # la diff entre todo los end y start de delivery es siempre la misma, por lo que me puedo quedar con la diff de cualquiera en la oportunidad\n",
    "    # con respecto al start me puedo quedar con el maximo y el minimo\n",
    "    df_datetime.loc[:,'diff_minmax_Planned_Delivery_Start_Date'] = (groups['Planned_Delivery_Start_Date'].max() - groups['Planned_Delivery_Start_Date'].min()).dt.days\n",
    "    df_datetime.loc[:,'diff_delivery'] = (df_datetime.loc[:,'Planned_Delivery_End_Date'] - df_datetime.loc[:,'Planned_Delivery_Start_Date']).dt.days\n",
    "    #cuanto tiempo paso desde la oportunidad se creo hasta el comienzo del delivery (me quedo con el mean pq cada producto tiene su propio start_delivery)\n",
    "    #df_datetime['diff_created_delivery'] = groups.apply(lambda x : (x.Planned_Delivery_Start_Date - x.Opportunity_Created_Date).mean().days)\n",
    "    df_datetime.loc[:,'diff_account_opportunity'] = (df_datetime.loc[:,'Opportunity_Created_Date'] - df_datetime.loc[:,'Account_Created_Date']).dt.days\n",
    "    df_datetime.loc[:,'is_account_first'] = df_datetime.loc[:,'Account_Created_Date'] > df_datetime.loc[:,'Opportunity_Created_Date']\n",
    "    df_datetime.loc[:,'has_quote_exp'] = df_datetime.loc[:,'Account_Created_Date'] > df_datetime.loc[:,'Opportunity_Created_Date']\n",
    "    \n",
    "    df_datetime.loc[:, 'has_Quote_Expiry_Date'] = ~ df_datetime.loc[:, 'Quote_Expiry_Date'].isnull()\n",
    "    df_datetime.drop('Quote_Expiry_Date', axis=1, inplace=True)\n",
    "    \n",
    "    # Cantidad de oportunidades creadas los 7 y 30 dias antes respectivamente\n",
    "    Opportunity_Created_Date = pd.Series(df_datetime.index, index=df_datetime.Opportunity_Created_Date).sort_index()\n",
    "    count_7d = Opportunity_Created_Date.rolling('7d').count() - 1\n",
    "    count_7d.index = Opportunity_Created_Date.values\n",
    "    count_7d = count_7d.reindex(df_datetime.index)\n",
    "    df_datetime.loc[:,'count_7d_oppotunity'] = count_7d\n",
    "    \n",
    "    Opportunity_Created_Date = pd.Series(df_datetime.index, index=df_datetime.Opportunity_Created_Date).sort_index()\n",
    "    count_30d = Opportunity_Created_Date.rolling('7d').count() - 1\n",
    "    count_30d.index = Opportunity_Created_Date.values\n",
    "    count_30d = count_30d.reindex(df_datetime.index)\n",
    "    df_datetime.loc[:,'count_7d_oppotunity'] = count_30d\n",
    "    \n",
    "    for col in df_datetime.select_dtypes(include='datetime').columns:\n",
    "        df_datetime.loc[:,col + '_weekofyear'] = df_datetime.loc[:,col].dt.weekofyear\n",
    "        #df_datetime = sin_cos_date(df_datetime, col + '_weekofyear', 52)\n",
    "        df_datetime.loc[:,col + '_year'] = df_datetime.loc[:,col].dt.year\n",
    "        df_datetime.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    if with_leackage:\n",
    "        df_target_leackage = (df_first.Sales_Contract_No == 'None').to_frame('Empty_Contract')\n",
    "    \n",
    "    df_cat_first = df_first.select_dtypes(include='object')\n",
    "    df_cat_first.loc[:, 'Empty_Source'] = df_cat_first.loc[:,'Source '].isnull()\n",
    "    df_cat_first.drop('Source ', axis=1, inplace=True)\n",
    "    \n",
    "    X = pd.concat([df_count, df_float, df_datetime, df_cat_first], axis=1)\n",
    "    \n",
    "    if with_leackage:\n",
    "        X = X.join(df_target_leackage)\n",
    "    #X = pd.DataFrame(index= df_first.index)\n",
    "    #X = X.join([df_count, df_float, df_datetime, df_cat_first])\n",
    "    return X\n",
    "\n",
    "def create_pipe(clf):\n",
    "    \n",
    "    numerical_transformer = MinMaxScaler()\n",
    "    categorical_transformer = TargetEncoder()\n",
    "    \n",
    "    num_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "    cat_cols = X.select_dtypes(include='object').columns\n",
    "    \n",
    "    preprocessor = make_column_transformer(\n",
    "        (numerical_transformer, num_cols),\n",
    "        (categorical_transformer, cat_cols),\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    pipe = make_pipeline(preprocessor, FunctionTransformer(lambda x: x.astype('float64')), clf)\n",
    "        \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/envs/tesis_env/lib/python3.8/site-packages/pandas/core/indexing.py:845: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/martin/anaconda3/envs/tesis_env/lib/python3.8/site-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/home/martin/anaconda3/envs/tesis_env/lib/python3.8/site-packages/pandas/core/frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('CompetenciaECI/Entrenamieto_ECI_2020.csv', index_col = 'Opportunity_ID')\n",
    "df_val = pd.read_csv('CompetenciaECI/Validacion_ECI_2020.csv', index_col = 'Opportunity_ID')\n",
    "df_train = discard_not_closed(df_train)\n",
    "df_y = df_train.pop('Stage')\n",
    "y = df_y.groupby(df_y.index).first()\n",
    "y = (y=='Closed Won') * 1 \n",
    "X = process_data(df_train, with_leackage=True)\n",
    "X_val = process_data(df_val, with_leackage=True)\n",
    "#X, X_val = X.align(X_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(create_pipe(XGBClassifier).steps[:-1])\n",
    "r = pipe.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pipe.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(r).sum(), (r == None).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1.0,2,3], dtype=object)\n",
    "np.array(list(a)).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning de XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seteo de cantidad de iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "class XGBoostWithEarlyStop(BaseEstimator):\n",
    "    def __init__(self, early_stopping_rounds=50, test_size=0.2, \n",
    "                 eval_metric='auc', **estimator_params):\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.test_size = test_size\n",
    "        self.eval_metric = eval_metric\n",
    "        if self.estimator is not None:\n",
    "            self.set_params(**estimator_params)\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        return self.estimator.set_params(**params)\n",
    "\n",
    "    def get_params(self, **params):\n",
    "        return self.estimator.get_params()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=self.test_size)\n",
    "        self.estimator.fit(x_train, y_train, \n",
    "                           early_stopping_rounds=self.early_stopping_rounds, \n",
    "                           eval_metric=self.eval_metric, eval_set=[(x_val, y_val)])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "class XGBoostClassifierWithEarlyStop(XGBoostWithEarlyStop):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.estimator = XGBClassifier(learning_rate =0.1, \n",
    "                                       n_estimators=1000,\n",
    "                                       objective= 'binary:logistic',\n",
    "                                       seed=4)\n",
    "        super(XGBoostClassifierWithEarlyStop, self).__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp(pipe):\n",
    "    feat_imp = pd.Series(pipe.named_steps['clf'].get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances', figsize=(13,4))\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_xgb(clf, param_grid):\n",
    "    for k in list(param_grid.keys()):\n",
    "        param_grid[f'clf__{k}'] = param_grid.pop(k)\n",
    "    pipe = create_pipe(clf)\n",
    "    gsearch = GridSearchCV(estimator = pipe,\n",
    "                           scoring='neg_log_loss',\n",
    "                           param_grid=param_grid,\n",
    "                           n_jobs=4,\n",
    "                           cv=5)\n",
    "    \n",
    "    gsearch.fit(X_train, y_train)\n",
    "    print(gsearch.cv_results_, gsearch.best_params_, gsearch.best_score_)\n",
    "    feat_imp(pipe)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.98287\n",
      "Will train until validation_0-auc hasn't improved in 50 rounds.\n",
      "[1]\tvalidation_0-auc:0.98253\n",
      "[2]\tvalidation_0-auc:0.98266\n",
      "[3]\tvalidation_0-auc:0.98343\n",
      "[4]\tvalidation_0-auc:0.98398\n",
      "[5]\tvalidation_0-auc:0.98478\n",
      "[6]\tvalidation_0-auc:0.98576\n",
      "[7]\tvalidation_0-auc:0.98638\n",
      "[8]\tvalidation_0-auc:0.98618\n",
      "[9]\tvalidation_0-auc:0.98671\n",
      "[10]\tvalidation_0-auc:0.98684\n",
      "[11]\tvalidation_0-auc:0.98691\n",
      "[12]\tvalidation_0-auc:0.98689\n",
      "[13]\tvalidation_0-auc:0.98660\n",
      "[14]\tvalidation_0-auc:0.98703\n",
      "[15]\tvalidation_0-auc:0.98620\n",
      "[16]\tvalidation_0-auc:0.98654\n",
      "[17]\tvalidation_0-auc:0.98654\n",
      "[18]\tvalidation_0-auc:0.98671\n",
      "[19]\tvalidation_0-auc:0.98648\n",
      "[20]\tvalidation_0-auc:0.98666\n",
      "[21]\tvalidation_0-auc:0.98654\n",
      "[22]\tvalidation_0-auc:0.98723\n",
      "[23]\tvalidation_0-auc:0.98723\n",
      "[24]\tvalidation_0-auc:0.98774\n",
      "[25]\tvalidation_0-auc:0.98769\n",
      "[26]\tvalidation_0-auc:0.98784\n",
      "[27]\tvalidation_0-auc:0.98803\n",
      "[28]\tvalidation_0-auc:0.98808\n",
      "[29]\tvalidation_0-auc:0.98803\n",
      "[30]\tvalidation_0-auc:0.98803\n",
      "[31]\tvalidation_0-auc:0.98823\n",
      "[32]\tvalidation_0-auc:0.98833\n",
      "[33]\tvalidation_0-auc:0.98840\n",
      "[34]\tvalidation_0-auc:0.98879\n",
      "[35]\tvalidation_0-auc:0.98874\n",
      "[36]\tvalidation_0-auc:0.98895\n",
      "[37]\tvalidation_0-auc:0.98911\n",
      "[38]\tvalidation_0-auc:0.98916\n",
      "[39]\tvalidation_0-auc:0.98914\n",
      "[40]\tvalidation_0-auc:0.98913\n",
      "[41]\tvalidation_0-auc:0.98896\n",
      "[42]\tvalidation_0-auc:0.98890\n",
      "[43]\tvalidation_0-auc:0.98879\n",
      "[44]\tvalidation_0-auc:0.98867\n",
      "[45]\tvalidation_0-auc:0.98883\n",
      "[46]\tvalidation_0-auc:0.98892\n",
      "[47]\tvalidation_0-auc:0.98895\n",
      "[48]\tvalidation_0-auc:0.98893\n",
      "[49]\tvalidation_0-auc:0.98904\n",
      "[50]\tvalidation_0-auc:0.98899\n",
      "[51]\tvalidation_0-auc:0.98890\n",
      "[52]\tvalidation_0-auc:0.98904\n",
      "[53]\tvalidation_0-auc:0.98909\n",
      "[54]\tvalidation_0-auc:0.98908\n",
      "[55]\tvalidation_0-auc:0.98910\n",
      "[56]\tvalidation_0-auc:0.98904\n",
      "[57]\tvalidation_0-auc:0.98905\n",
      "[58]\tvalidation_0-auc:0.98901\n",
      "[59]\tvalidation_0-auc:0.98910\n",
      "[60]\tvalidation_0-auc:0.98913\n",
      "[61]\tvalidation_0-auc:0.98916\n",
      "[62]\tvalidation_0-auc:0.98905\n",
      "[63]\tvalidation_0-auc:0.98911\n",
      "[64]\tvalidation_0-auc:0.98902\n",
      "[65]\tvalidation_0-auc:0.98892\n",
      "[66]\tvalidation_0-auc:0.98891\n",
      "[67]\tvalidation_0-auc:0.98889\n",
      "[68]\tvalidation_0-auc:0.98886\n",
      "[69]\tvalidation_0-auc:0.98878\n",
      "[70]\tvalidation_0-auc:0.98873\n",
      "[71]\tvalidation_0-auc:0.98871\n",
      "[72]\tvalidation_0-auc:0.98870\n",
      "[73]\tvalidation_0-auc:0.98860\n",
      "[74]\tvalidation_0-auc:0.98855\n",
      "[75]\tvalidation_0-auc:0.98853\n",
      "[76]\tvalidation_0-auc:0.98861\n",
      "[77]\tvalidation_0-auc:0.98855\n",
      "[78]\tvalidation_0-auc:0.98855\n",
      "[79]\tvalidation_0-auc:0.98847\n",
      "[80]\tvalidation_0-auc:0.98843\n",
      "[81]\tvalidation_0-auc:0.98856\n",
      "[82]\tvalidation_0-auc:0.98858\n",
      "[83]\tvalidation_0-auc:0.98872\n",
      "[84]\tvalidation_0-auc:0.98872\n",
      "[85]\tvalidation_0-auc:0.98875\n",
      "[86]\tvalidation_0-auc:0.98876\n",
      "[87]\tvalidation_0-auc:0.98881\n",
      "[88]\tvalidation_0-auc:0.98879\n",
      "[89]\tvalidation_0-auc:0.98869\n",
      "[90]\tvalidation_0-auc:0.98869\n",
      "[91]\tvalidation_0-auc:0.98869\n",
      "[92]\tvalidation_0-auc:0.98868\n",
      "[93]\tvalidation_0-auc:0.98861\n",
      "[94]\tvalidation_0-auc:0.98859\n",
      "[95]\tvalidation_0-auc:0.98866\n",
      "[96]\tvalidation_0-auc:0.98852\n",
      "[97]\tvalidation_0-auc:0.98853\n",
      "[98]\tvalidation_0-auc:0.98849\n",
      "[99]\tvalidation_0-auc:0.98849\n",
      "[100]\tvalidation_0-auc:0.98852\n",
      "[101]\tvalidation_0-auc:0.98844\n",
      "[102]\tvalidation_0-auc:0.98846\n",
      "[103]\tvalidation_0-auc:0.98850\n",
      "[104]\tvalidation_0-auc:0.98855\n",
      "[105]\tvalidation_0-auc:0.98856\n",
      "[106]\tvalidation_0-auc:0.98846\n",
      "[107]\tvalidation_0-auc:0.98850\n",
      "[108]\tvalidation_0-auc:0.98845\n",
      "[109]\tvalidation_0-auc:0.98847\n",
      "[110]\tvalidation_0-auc:0.98851\n",
      "[111]\tvalidation_0-auc:0.98845\n",
      "Stopping. Best iteration:\n",
      "[61]\tvalidation_0-auc:0.98916\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 Pipeline(memory=None,\n",
       "                          steps=[('columntransformer',\n",
       "                                  ColumnTransformer(n_jobs=None,\n",
       "                                                    remainder='drop',\n",
       "                                                    sparse_threshold=0.3,\n",
       "                                                    transformer_weights=None,\n",
       "                                                    transformers=[('minmaxscaler',\n",
       "                                                                   MinMaxScaler(copy=True,\n",
       "                                                                                feature_range=(0,\n",
       "                                                                                               1)),\n",
       "                                                                   Index([                            'nb_products',\n",
       "                                 ('ASP', 'mean'),\n",
       "                                  ('ASP', 'std'),\n",
       "                                  ('ASP', 'min'),\n",
       "                                  ('ASP', 'max'),\n",
       "                                  ('ASP', 'sum'...\n",
       "                                                interaction_constraints=None,\n",
       "                                                learning_rate=0.1,\n",
       "                                                max_delta_step=0, max_depth=6,\n",
       "                                                min_child_weight=1, missing=nan,\n",
       "                                                monotone_constraints=None,\n",
       "                                                n_estimators=1000, n_jobs=0,\n",
       "                                                num_parallel_tree=1,\n",
       "                                                objective='binary:logistic',\n",
       "                                                random_state=4, reg_alpha=0,\n",
       "                                                reg_lambda=1,\n",
       "                                                scale_pos_weight=1, seed=4,\n",
       "                                                subsample=1, tree_method=None,\n",
       "                                                validate_parameters=False,\n",
       "                                                verbosity=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = create_pipe(XGBoostClassifierWithEarlyStop())\n",
    "pipe.fit(X,y)\n",
    "# 370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058641516936489044\n"
     ]
    }
   ],
   "source": [
    "print( log_loss(y, pipe.predict_proba(X)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tuneo de Profundidad del arbol y ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cv_xgb(XGBClassifier(learning_rate =0.1, \n",
    "              n_estimators=370, \n",
    "              max_depth=5,\n",
    "              min_child_weight=1, \n",
    "              gamma=0, \n",
    "              subsample=0.8, \n",
    "              colsample_bytree=0.8,\n",
    "              objective= 'binary:logistic', \n",
    "              nthread=4, \n",
    "              seed=27),\n",
    "       { \n",
    "           'max_depth':range(3,4,2), \n",
    "           'min_child_weight':range(1,2,1)\n",
    "       }\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tuneo de gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb(XGBClassifier(learning_rate =0.1, \n",
    "              n_estimators=370, \n",
    "              max_depth=4,\n",
    "              min_child_weight=4, \n",
    "              gamma=0, \n",
    "              subsample=0.8, \n",
    "              colsample_bytree=0.8,\n",
    "              objective= 'binary:logistic', \n",
    "              nthread=4, \n",
    "              seed=27),\n",
    "       { \n",
    "           'gamma':[i/10.0 for i in range(0,5)]\n",
    "       }\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tuneo de porcion del datset de entrenamiento usado para entrenar cada arbol y porcentaje de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb(XGBClassifier(learning_rate =0.1, \n",
    "              n_estimators=370, \n",
    "              max_depth=4,\n",
    "              min_child_weight=4, \n",
    "              gamma=0, \n",
    "              subsample=0.8, \n",
    "              colsample_bytree=0.8,\n",
    "              objective= 'binary:logistic', \n",
    "              nthread=4, \n",
    "              seed=27),\n",
    "       {\n",
    " 'subsample':[i/10.0 for i in range(6,11)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,11)]\n",
    "}\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tuneo de alpha para regularizacion L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb(XGBClassifier(learning_rate =0.1, \n",
    "              n_estimators=370, \n",
    "              max_depth=4,\n",
    "              min_child_weight=4, \n",
    "              gamma=0, \n",
    "              subsample=0.7, \n",
    "              colsample_bytree=0.8,\n",
    "              objective= 'binary:logistic', \n",
    "              nthread=4, \n",
    "              seed=27),\n",
    "             {'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "clf = XGBClassifier(learning_rate =0.1, \n",
    "              n_estimators=370, \n",
    "              max_depth=4,\n",
    "              min_child_weight=4, \n",
    "              gamma=0, \n",
    "              subsample=0.7, \n",
    "              colsample_bytree=0.8,\n",
    "              objective= 'binary:logistic', \n",
    "              nthread=4, \n",
    "              seed=27)\n",
    "#clf = LogisticRegression(max_iter=1000)\n",
    "pipe = create_pipe(clf)\n",
    "pipe.fit(X,y)\n",
    "#, eval_set= [(X_test, y_test)],early_stopping_rounds= 50)\n",
    "print( log_loss(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "( log_loss(y, pipe.predict_proba(X)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB sin tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fit clf no training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n",
    "clf = XGBClassifier(gamma=.1, \n",
    "                        n_estimators=200, \n",
    "                        subsample=.9,\n",
    "                        colsample_bytree=.9,\n",
    "                        eval_metric='logloss')\n",
    "clf.fit(X_train,y_train)\n",
    "#print(clf.score(X_train,y_train))\n",
    "print( log_loss(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "print( log_loss(y_test, clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(clf, max_num_features=20, importance_type='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = -1 * cross_val_score(xgb.XGBClassifier(), X, y,\n",
    "                              cv=10,\n",
    "                              scoring='neg_log_loss')\n",
    "print(f'mean: {scores.mean()} std: {scores.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=400)\n",
    "scores = -1 * cross_val_score(clf, X, y,\n",
    "                              cv=10,\n",
    "                              scoring='neg_log_loss')\n",
    "print(f'mean: {scores.mean()} std: {scores.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare results to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict_proba(X_val)[:,1]\n",
    "results = pd.DataFrame(data = {'Opportunity_ID': X_val.index, 'score': y_pred})   \n",
    "results.to_csv(f'results/result{datetime.now()}.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
