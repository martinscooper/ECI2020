{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from category_encoders import TargetEncoder\n",
    "sns.set()\n",
    "pd.set_option('display.max_rows', 90)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = ['Pricing, Delivery_Terms_Quote_Appr', 'Pricing, Delivery_Terms_Approved', \n",
    "           'Bureaucratic_Code_0_Approval','Bureaucratic_Code_0_Approved']\n",
    "\n",
    "categorical_cols = ['Region','Territory','Bureaucratic_Code', \n",
    "                'Source ','Billing_Country','Account_Name',\n",
    "                'Opportunity_Name', 'Account_Owner',\n",
    "               'Opportunity_Owner','Account_Type', 'Opportunity_Type', \n",
    "                'Quote_Type', 'Delivery_Terms', 'Brand', 'Product_Type', \n",
    "                'Size','Product_Category_B', 'Currency', 'Last_Modified_By',\n",
    "                'Product_Family', 'Product_Name', 'ASP_Currency',\n",
    "                'ASP_(converted)_Currency','Delivery_Quarter',\n",
    "                'Total_Taxable_Amount_Currency',\n",
    "                'Prod_Category_A', 'Total_Amount_Currency'\n",
    "               ]\n",
    "float_cols = ['ASP', 'ASP_(converted)', \n",
    "              'TRF', 'Total_Amount', \n",
    "              'Total_Taxable_Amount', 'ID']\n",
    "\n",
    "\n",
    "datetime_cols = ['Account_Created_Date', 'Opportunity_Created_Date',\n",
    "             'Last_Activity','Quote_Expiry_Date', 'Last_Modified_Date',\n",
    "             'Planned_Delivery_Start_Date', 'Planned_Delivery_End_Date',\n",
    "             'Actual_Delivery_Date', \n",
    "            ]\n",
    "other_cols = ['Month', 'Delivery_Year', 'Price']\n",
    "\n",
    "target_col = ['Stage']\n",
    "    \n",
    "######### debe ser eliminada ya que aumentara el desempeÃ±o del predictor ahora pero en\n",
    "######### un caso real nunca se tendra el valor de esta variable ya que parece ser seteado\n",
    "######### una vez que la oportunidad resulta en exito.\n",
    "target_leakage = ['Sales_Contract_No']\n",
    "\n",
    "def discard_not_closed(df):\n",
    "    return df.loc[df['Stage'].apply(lambda x: x in ['Closed Won', 'Closed Lost'])]\n",
    "\n",
    "def process_data(df):\n",
    "\n",
    "    # Tipos de variables segun la decr del dataset\n",
    "    \n",
    "    # Saco algunas columnas\n",
    "    # demasiados categorias\n",
    "    many_cats = ['Opportunity_Name']\n",
    "    # por gran mayoria de nones:\n",
    "    nones_may = ['Product_Type', 'Product_Category_B','Currency', 'Size', 'Brand']\n",
    "    # por similitud a ASP_Currency\n",
    "    sim_vals = ['Total_Taxable_Amount_Currency'] \n",
    "    # posee un solo valor o casi\n",
    "    one_val = ['Prod_Category_A', 'ASP_(converted)_Currency','Quote_Type', 'Submitted_for_Approval']\n",
    "    # Id columns\n",
    "    id_cols = ['ID']\n",
    "    #datetime nones\n",
    "    none_date = ['Last_Activity','Actual_Delivery_Date']\n",
    "\n",
    "    \n",
    "    \n",
    "    global categorical_cols, float_cols, datetime_cols\n",
    "    categorical_cols = list(set(categorical_cols) - set(nones_may + sim_vals + one_val  + many_cats))\n",
    "    float_cols = list(set(float_cols) - set(id_cols + target_leakage))\n",
    "    datetime_cols = list(set(datetime_cols) - set(none_date))\n",
    "\n",
    "    df = df.loc[:, categorical_cols + binary_cols + float_cols + datetime_cols]\n",
    "    \n",
    "    # Completo Nones\n",
    "    df.loc[df['Territory']=='None', 'Territory'] = df.loc[df['Territory']=='None', 'Region'] \n",
    "\n",
    "    # Completo NaN\n",
    "    df['ASP_(converted)'].fillna(0, inplace=True)\n",
    "    df['ASP'].fillna(0, inplace=True)\n",
    "    df['Total_Amount'].fillna(0, inplace=True)\n",
    "\n",
    "    # Convierto los tipos de variables segun la descr. del df\n",
    "    df[binary_cols] = df[binary_cols].astype('bool')\n",
    "    df[categorical_cols] = df[categorical_cols].astype('category')\n",
    "    df[float_cols] = df[float_cols].astype('float64')\n",
    "    #df[target_leakage] = df[target_leakage].astype('int')\n",
    "    for col in datetime_cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    #completo end delivery date con la suma entre Planned_Delivery_End_Date y el promedio de su diferencia con Planned_Delivery_End_Date\n",
    "    diff_delivery = df['Planned_Delivery_End_Date'] - df['Planned_Delivery_Start_Date']\n",
    "    start_plus_mean = df.loc[df['Planned_Delivery_End_Date'].isnull(),'Planned_Delivery_Start_Date'] + diff_delivery.mean()\n",
    "    df.loc[df['Planned_Delivery_End_Date'].isnull(), 'Planned_Delivery_End_Date'] = start_plus_mean\n",
    "    \n",
    "    \n",
    "    #preparo los grupos segun la oportunidad y\n",
    "    # un grupo con la primer columna de cada grupo\n",
    "    groups = df.groupby(df.index)\n",
    "    df_first = df.reset_index().drop_duplicates(subset='Opportunity_ID', keep='first').set_index('Opportunity_ID')\n",
    "    df_count = groups.size().to_frame('nb_products')\n",
    "    df_count['only_one_product'] = df_count['nb_products'] == 1 \n",
    "    \n",
    "    \n",
    "    groups_float = groups[df.select_dtypes(include=['float','int']).columns]\n",
    "    df_float = groups_float.agg(['mean','std','min','max','sum']).fillna(0)\n",
    "    \n",
    "    def sin_cos_date(df, col, total):\n",
    "        df[col + '_sin'] = np.sin(2 * np.pi * df[col] / total)\n",
    "        df[col + '_cos'] = np.cos(2 * np.pi * df[col] / total)\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    datetime_cols = df.select_dtypes(include='datetime').columns\n",
    "    df_datetime = df_first[datetime_cols]\n",
    "\n",
    "    #datetime vars engineer\n",
    "    # la diff entre todo los end y start de delivery es siempre la misma, por lo que me puedo quedar con la diff de cualquiera en la oportunidad\n",
    "    # con respecto al start me puedo quedar con el maximo y el minimo\n",
    "    df_datetime.loc[:,'diff_minmax_Planned_Delivery_Start_Date'] = (groups['Planned_Delivery_Start_Date'].max() - groups['Planned_Delivery_Start_Date'].min()).dt.days\n",
    "    df_datetime.loc[:,'diff_delivery'] = (df_datetime.loc[:,'Planned_Delivery_End_Date'] - df_datetime.loc[:,'Planned_Delivery_Start_Date']).dt.days\n",
    "    #cuanto tiempo paso desde la oportunidad se creo hasta el comienzo del delivery (me quedo con el mean pq cada producto tiene su propio start_delivery)\n",
    "    #df_datetime['diff_created_delivery'] = groups.apply(lambda x : (x.Planned_Delivery_Start_Date - x.Opportunity_Created_Date).mean().days)\n",
    "    df_datetime.loc[:,'diff_account_opportunity'] = (df_datetime.loc[:,'Opportunity_Created_Date'] - df_datetime.loc[:,'Account_Created_Date']).dt.days\n",
    "    df_datetime.loc[:,'is_account_first'] = df_datetime.loc[:,'Account_Created_Date'] > df_datetime.loc[:,'Opportunity_Created_Date']\n",
    "    df_datetime.loc[:,'has_quote_exp'] = df_datetime.loc[:,'Account_Created_Date'] > df_datetime.loc[:,'Opportunity_Created_Date']\n",
    "    \n",
    "    df_datetime.loc[:, 'has_Quote_Expiry_Date'] = ~ df_datetime.loc[:, 'Quote_Expiry_Date'].isnull()\n",
    "    df_datetime.drop('Quote_Expiry_Date', axis=1, inplace=True)\n",
    "    \n",
    "    for col in df_datetime.select_dtypes(include='datetime').columns:\n",
    "        df_datetime.loc[:,col + '_weekofyear'] = df_datetime[col].dt.weekofyear\n",
    "        #df_datetime = sin_cos_date(df_datetime, col + '_weekofyear', 52)\n",
    "        df_datetime.loc[:,col + '_year'] = df_datetime[col].dt.year\n",
    "        df_datetime.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    #if with_target_leakage:\n",
    "    #    df_target_leackage = (df_first.Sales_Contract_No == 'None').to_frame('Empty_Contract')\n",
    "    \n",
    "    df_cat_news = df_first['Source '].isnull().to_frame('Empty_Source')\n",
    "    df_cat_first = df_first.select_dtypes(include='category')\n",
    "    df_cat_first.pop('Source ')\n",
    "    \n",
    "    X = pd.DataFrame(index= df_first.index)\n",
    "    X = X.join([df_count, df_float, df_datetime, df_cat_news, df_cat_first])\n",
    "    #[df_count, df_float, df_datetime, df_cat_news, df_cat_first]\n",
    "    return X\n",
    "\n",
    "def create_pipe(clf):\n",
    "    \n",
    "    numerical_transformer = MinMaxScaler()\n",
    "    categorical_transformer = TargetEncoder()\n",
    "\n",
    "    numerical_cols = X.select_dtypes(include='float').columns\n",
    "    categorical_cols = X.select_dtypes(include='category').columns\n",
    "\n",
    "    preprocessor_cat = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    preprocessor_num = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, X.columns),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    clf = XGBClassifier(learning_rate =0.1, \n",
    "                        n_estimators=1000, \n",
    "                        max_depth=3,\n",
    "                        min_child_weight=1, \n",
    "                        gamma=0, \n",
    "                        subsample=1, \n",
    "                        colsample_bytree=0.6,\n",
    "                        objective= 'binary:logistic', \n",
    "                        nthread=4,\n",
    "                        reg_alpha=.01,\n",
    "                        seed=27)\n",
    "\n",
    "    pipe = Pipeline(steps=[('preprocessor_cat', preprocessor_cat),\n",
    "                           ('preprocessor_num', preprocessor_num),\n",
    "                           ('model', clf)])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('CompetenciaECI/Entrenamieto_ECI_2020.csv', index_col = 'Opportunity_ID')\n",
    "df_val = pd.read_csv('CompetenciaECI/Validacion_ECI_2020.csv', index_col = 'Opportunity_ID')\n",
    "df_train = discard_not_closed(df_train)\n",
    "df_y = df_train.pop('Stage')\n",
    "y = df_y.groupby(df_y.index).first()\n",
    "y = (y=='Closed Won') * 1 \n",
    "X = process_data(df_train)\n",
    "X_val = process_data(df_val)\n",
    "X, X_val = X.align(X_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(preprocessor.fit_transform(X,y), columns=X.columns, index=X.index)\n",
    "X_val = pd.DataFrame(pxreprocessor.transform(X_val), columns=X.columns, index=X_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning de XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seteo de cantidad de iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp(pipe):\n",
    "    feat_imp = pd.Series(pipe.named_steps['model'].get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances', figsize=(13,4))\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "target = 'Stage'\n",
    "train = pd.concat([X,y.to_frame(target)], axis=1)\n",
    "predictors = [x for x in train.columns if x not in [target]]\n",
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    seed=27)\n",
    "pipe = create_pipe(xgb1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_xgb(clf):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tuneo de Profundidad del arbol y ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-d52f73d0285f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cv_xgb(XGBClassifier(learning_rate =0.1, \n\u001b[0m\u001b[1;32m      2\u001b[0m               \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv_xgb' is not defined"
     ]
    }
   ],
   "source": [
    "cv_xgb(XGBClassifier(learning_rate =0.1, \n",
    "              n_estimators=140, \n",
    "              max_depth=5,\n",
    "              min_child_weight=1, \n",
    "              gamma=0, \n",
    "              subsample=0.8, \n",
    "              colsample_bytree=0.8,\n",
    "              objective= 'binary:logistic', \n",
    "              nthread=4, \n",
    "              seed=27),\n",
    "       param_test1 = { \n",
    "           'max_depth':range(3,10,1), \n",
    "           'min_child_weight':range(1,6,1)\n",
    "       }\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tuneo de gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "cv_xgb(XGBClassifier(learning_rate =0.1, \n",
    "              n_estimators=140, \n",
    "              max_depth=5,\n",
    "              min_child_weight=1, \n",
    "              gamma=0, \n",
    "              subsample=0.8, \n",
    "              colsample_bytree=0.8,\n",
    "              objective= 'binary:logistic', \n",
    "              nthread=4, \n",
    "              seed=27),\n",
    "       param_test1 = { \n",
    "           'max_depth':range(3,10,1), \n",
    "           'min_child_weight':range(1,6,1)\n",
    "       }\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tuneo de porcion del datset de entrenamiento usado para entrenar cada arbol y porcentaje de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'subsample':[i/10.0 for i in range(6,11)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,11)]\n",
    "}\n",
    "cv_xgb(XGBClassifier(learning_rate =0.1, \n",
    "              n_estimators=140, \n",
    "              max_depth=5,\n",
    "              min_child_weight=1, \n",
    "              gamma=0, \n",
    "              subsample=0.8, \n",
    "              colsample_bytree=0.8,\n",
    "              objective= 'binary:logistic', \n",
    "              nthread=4, \n",
    "              seed=27),\n",
    "       param_test1 = { \n",
    "           'max_depth':range(3,10,1), \n",
    "           'min_child_weight':range(1,6,1)\n",
    "       }\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tuneo de alpha para regularizacion L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "cv_xgb(XGBClassifier(learning_rate =0.1, \n",
    "              n_estimators=140, \n",
    "              max_depth=5,\n",
    "              min_child_weight=1, \n",
    "              gamma=0, \n",
    "              subsample=0.8, \n",
    "              colsample_bytree=0.8,\n",
    "              objective= 'binary:logistic', \n",
    "              nthread=4, \n",
    "              seed=27),\n",
    "       param_test1 = { \n",
    "           'max_depth':range(3,10,1), \n",
    "           'min_child_weight':range(1,6,1)\n",
    "       }\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "print( log_loss(y_test, pipe.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "clf = XGBClassifier(learning_rate =0.1, \n",
    "                    n_estimators=1000, \n",
    "                    max_depth=3,\n",
    "                    min_child_weight=1, \n",
    "                    gamma=0, \n",
    "                    subsample=1, \n",
    "                    colsample_bytree=0.6,\n",
    "                    objective= 'binary:logistic', \n",
    "                    nthread=4,\n",
    "                    reg_alpha=.01,\n",
    "                    seed=27)\n",
    "clf.fit(X_train,y_train, \n",
    "        eval_set=[(X_test,y_test)], \n",
    "        eval_metric='logloss', \n",
    "        early_stopping_rounds=20,\n",
    "       verbose=False)\n",
    "print( log_loss(y, clf.predict_proba(X)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB sin tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fit model no training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n",
    "clf = xgb.XGBClassifier(gamma=3.5, \n",
    "                        n_estimators=1000, \n",
    "                        silent=1, \n",
    "                        eta=.1,\n",
    "                        subsample=.8,\n",
    "                        colsample_bytree=.8,\n",
    "                        eval_metric='logloss')\n",
    "clf.fit(X_train,y_train)\n",
    "#print(clf.score(X_train,y_train))\n",
    "print( log_loss(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "print( log_loss(y_test, clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(clf, max_num_features=20, importance_type='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = -1 * cross_val_score(xgb.XGBClassifier(), X, y,\n",
    "                              cv=10,\n",
    "                              scoring='neg_log_loss')\n",
    "print(f'mean: {scores.mean()} std: {scores.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=400)\n",
    "scores = -1 * cross_val_score(clf, X, y,\n",
    "                              cv=10,\n",
    "                              scoring='neg_log_loss')\n",
    "print(f'mean: {scores.mean()} std: {scores.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare results to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)\n",
    "y_pred = clf.predict_proba(X_val)[:,1]\n",
    "results = pd.DataFrame(data = {'Opportunity_ID': X_val.index, 'score': y_pred})   \n",
    "results.to_csv(f'results/result{datetime.now()}.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
